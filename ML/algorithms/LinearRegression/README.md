# Simple Linear Regression

![6772339_1608277678_simple](https://github.com/avs-abhishek123/Machine-Learning/blob/main/images/sample_images/6772339_1608277678_simple.jpg?raw=true)

<hr style ="border: 10px solid purple; border-radius: 5px;">

| Content | Link |
| :---: | :---: |
| Simple Linear Regression | [Notebook Link](https://github.com/avs-abhishek123/Machine-Learning/blob/main/ML/algorithms/LinearRegression/simple_linear_regression.ipynb) |
| Simple Linear Regression Kaggle Notebook | [My Kaggle Notebook Link](https://www.kaggle.com/code/abhishek14398/simple-linear-regression) |
| Linear Regression Gradient Descent | [Notebook Link](https://github.com/avs-abhishek123/Machine-Learning/blob/main/ML/algorithms/LinearRegression/linear_regression_gradient_descent.py) |
| Linear Regression Normal Equation | [Notebook Link](https://github.com/avs-abhishek123/Machine-Learning/blob/main/ML/algorithms/LinearRegression/linear_regression_normal_equation.py) |

<hr style ="border: 10px solid purple; border-radius: 5px;">

We can generate predictions using simple linear regression, a statistical technique for determining the connection between two variables. Typically, the two variables are written as y and x. X stands for the independent variable, or the variable that is utilised to predict the dependent variable. y stands for the dependent variable, often known as the result or output.

A line of best fit, or the regression line, will be generated by a straightforward linear regression model. You might be familiar with the idea of tracing the line of greatest fit through a data scatter plot. Consider a scatter plot that illustrates how years of experience impact earnings. Think of a line you may draw to indicate the pattern.

![simple_linear_regression_eq](https://github.com/avs-abhishek123/Machine-Learning/blob/main/images/simple_linear_regression_eq.png?raw=true)

The simple linear regression equation we'll employ is presented here. The regression line's y-intercept (ùú∑0), or where it will begin on the y-axis, is the constant. The slope and description of the link between the independent and dependent variables are provided by the beta coefficient (ùú∑1). The amount of change in the dependent variable for each change in the independent variable of one unit is represented by the coefficient, which can be either positive or negative.

<hr style ="border: 10px solid purple; border-radius: 5px;">

<h3 style="color: blue; font-size: 26px;">
    <b>
        <u>
            Making the Regression Line Calculation
        </u>
    </b>
</h3>

Fortunately, there are methods that we can use to quickly calculate the slope and intercept of the linear regression line even if we could spend all day estimating these values.

![](https://github.com/avs-abhishek123/Machine-Learning/blob/main/images/sample_images/regression_sample.png?raw=true)

We will apply the following formula to calculate the data's slope ùú∑1:

![](https://github.com/avs-abhishek123/Machine-Learning/blob/main/images/simple_linear_regression_eq2.png?raw=true)

We can use the following formula to calculate the intercept ùú∑0:

![](https://github.com/avs-abhishek123/Machine-Learning/blob/main/images/simple_linear_regression_eq3.png?raw=true)

<hr style ="border: 10px solid purple; border-radius: 5px;">


<h3 style="color: blue; font-size: 26px;">
    <b>
        <u>
            Calculating How Well The Regression Line Fits
        </u>
    </b>
</h3>

To determine how well our regression line fits the data, we want to calculate the correlation coefficient, commonly referred to just as R, and the coefficient of determination, otherwise known as R¬≤ (R squared).

- Coefficient of Determination (R¬≤) ‚Äî The percentage of variance explained by the independent variable (x) with values between 0 and 1. It cannot be negative because it is a square value. For example, if R¬≤ = 0.81, then this tells you that x explains 81% of the variance in y. Otherwise known as the ‚Äúgoodness of fit‚Äù.

- Correlation Coefficient (R) ‚Äî The degree of relationship or correlation between two variables (x and y in this case). R can range from -1 to 1 with values equal to 1 meaning a perfect positive correlation and values equal to -1 meaning a perfect negative correlation.

Below is the formula for Pearson‚Äôs correlation coefficient:

![](https://github.com/avs-abhishek123/Machine-Learning/blob/main/images/Pearson%20Correlation%20Coefficient%20(R).png?raw=true)

This formula has to be converted into Python code. We only need to square Pearson's correlation coefficient once to obtain the coefficient of determination.

The number of observations (data rows) needs to be re-stored in the variable N. The numerator and denominator of the formula will now be separated into two separate pieces. The correlation coefficient can then be given.

<hr style ="border: 10px solid purple; border-radius: 5px;">

![](https://inventionland.com/wp/wp-content/uploads/2015/09/National_Thank_You_Day-1.png)